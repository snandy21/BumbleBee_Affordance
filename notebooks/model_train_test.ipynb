{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import pickle\n",
    "import sys\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "#from ipdb import set_trace\n",
    "from datetime import datetime\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from ipdb import set_trace\n",
    "import argparse\n",
    "from torch.utils.data import Dataset\n",
    "from matplotlib import image\n",
    "from matplotlib import pyplot\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "import random\n",
    "plt.ion()   \n",
    "\n",
    "ROOT_DIR               = '../'\n",
    "NEW_DATASET_DIR        = ROOT_DIR + 'datasets/ntu_dataset/dataset_v3/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(vals, possible_vals):\n",
    "    if not isinstance(possible_vals, list): raise TypeError(\"provide possible_vals as a list\")\n",
    "    enc_vals = np.zeros([len(vals), len(possible_vals)])\n",
    "    for i, value in enumerate(vals):\n",
    "        if isinstance(possible_vals[0], float):\n",
    "            enc = np.where(abs(possible_vals-value)<1e-3)\n",
    "        else:\n",
    "            enc = np.where(possible_vals==value)\n",
    "        enc_vals[i,enc] = 1\n",
    "    return enc_vals\n",
    "\n",
    "def get_traffic_light_one_hot(traffic_light_state) : \n",
    "    if(traffic_light_state[0] == True) : \n",
    "        return 0\n",
    "    else : \n",
    "        return 1\n",
    "\n",
    "class Rescale(object):\n",
    "    def __init__(self, scalar):\n",
    "        self.scalar = scalar\n",
    "\n",
    "    def __call__(self, im):\n",
    "        w, h = [int(s*self.scalar) for s in im.size]\n",
    "        return transforms.Resize((h, w))(im)\n",
    "\n",
    "class Crop(object):\n",
    "    def __init__(self, box):\n",
    "        assert len(box) == 4\n",
    "        self.box = box\n",
    "\n",
    "    def __call__(self, im):\n",
    "        return im.crop(self.box)\n",
    "\n",
    "class Augment(object):\n",
    "    def __init__(self, seq):\n",
    "        self.seq = seq\n",
    "\n",
    "    def __call__(self, im):\n",
    "        return Image.fromarray(self.seq.augment_images([np.array(im)])[0])\n",
    "\n",
    "def load_checkpoint(filename) :\n",
    "    checkpoint = torch.load(filename)\n",
    "    model_state_dict = checkpoint['model_state_dict']\n",
    "    optimizer_state_dict = checkpoint['optimizer_state_dict']\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    return [model_state_dict, optimizer_state_dict, epoch, loss]\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, loss, filename):\n",
    "    print(\"Saving Checkpoint....\")\n",
    "    model_state_dict = model.state_dict()\n",
    "    optimizer_state_dict = optimizer.state_dict()\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model_state_dict,\n",
    "            'optimizer_state_dict': optimizer_state_dict,\n",
    "            'loss': loss\n",
    "            }, filename)\n",
    "    print(\"Saving Checkpoint Done\")\n",
    "\n",
    "def display_image(image) : \n",
    "    plt.show(image)\n",
    "    \n",
    "def display_torch_im(tensor) : \n",
    "    print(tensor.size())\n",
    "    tensor_img_0 = tensor[0].squeeze(0)\n",
    "    print(tensor_img_0.size())\n",
    "    plt.imshow(tensor_img_0.permute(1, 2, 0))\n",
    "    plt.show()\n",
    "    \n",
    "def load_checkpoint_weights(checkpoint, model) :\n",
    "    print(\"checkpoint name : \", checkpoint)\n",
    "    _model_state_dict, _optimizer_state_dict, _epoch, _loss = load_checkpoint(checkpoint)\n",
    "    model.load_state_dict(_model_state_dict)\n",
    "\n",
    "def resume_model(checkpoint, model, optimizer) : \n",
    "    print(\"checkpoint name : \", checkpoint)\n",
    "    _model_state_dict, _optimizer_state_dict, _epoch, _loss = load_checkpoint(checkpoint)\n",
    "    #print(_model_state_dict)\n",
    "    model.load_state_dict(_model_state_dict)\n",
    "    optimizer.load_state_dict(_optimizer_state_dict)\n",
    "    print(\"resume model succesful\")\n",
    "\n",
    "def plot_tensor_image(tensor_im) :\n",
    "    plt.imshow(tensor_im.permute(1, 2, 0))\n",
    "    plt.show()\n",
    "\n",
    "def get_train_test_split(image_dir_path): \n",
    "    image_list = os.listdir(image_dir_path)\n",
    "    file_name = []\n",
    "    for file in image_list:\n",
    "        file_name.append(os.path.splitext(file)[0])\n",
    "    random.shuffle(file_name) \n",
    "\n",
    "    split_1 = int(0.8 * len(file_name))\n",
    "    train_filenames = file_name[:split_1]\n",
    "    test_filenames = file_name[split_1:]\n",
    "    \n",
    "    return train_filenames, test_filenames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, dataset_sizes, criterion_dict, criterion_vec, optimizer, scheduler, num_epochs=30):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc       = 0.0\n",
    "    best_loss      = 0.0\n",
    "    epoch_acc      = 0.0\n",
    "    num_outputs    = len(criterion_vec)\n",
    "    \n",
    "    train_loss_epoch = []\n",
    "    val_loss_epoch   = []\n",
    "     \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for i, data in enumerate(dataloaders[phase]) :\n",
    "                inputs_old     = data[0]\n",
    "                labels         = data[1]\n",
    "                inputs, labels = convert_inputs(inputs_old ,labels)\n",
    "                inputs         = inputs.to(device)\n",
    "                labels         = labels.to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # forward, track history if only in train\n",
    "                current_batch_size = inputs.size(0)\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    regression_preds, classification_preds = \\\n",
    "                              custom_preds(outputs, current_batch_size, num_outputs)\n",
    "                    #custom loss for regression + classification head\n",
    "                    loss, losses    = custom_loss(outputs, labels, criterion_dict, criterion_vec)\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        if(i%20 == 0) :\n",
    "                            print()\n",
    "                            print_model_output(\"preds\", regression_preds, classification_preds)\n",
    "                            print_model_label(\"labels\", labels)\n",
    "                            print(\"losses : \", losses)\n",
    "                            print(\"loss :\", loss.item())\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                if(phase == 'val') :\n",
    "                    if(i%10==0) :\n",
    "                        print_model_output(\"preds\", regression_preds, classification_preds)\n",
    "                        print_model_label(\"labels\", labels)\n",
    "                        print(\"loss :\", loss.item())\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            if(phase == 'train') :\n",
    "                train_loss_epoch.append(epoch_loss)\n",
    "            elif(phase == 'val') :\n",
    "                val_loss_epoch.append(epoch_loss)\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            #if phase == 'val' and epoch_acc > best_acc:\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'train'  :\n",
    "                checkpoint_name = ROOT_DIR + 'checkpoints/model_' + str(epoch) + '.tar'\n",
    "                save_checkpoint(model, optimizer, epoch, epoch_loss, checkpoint_name)\n",
    "        print(\"\\n*********************************\")\n",
    "        print(train_loss_epoch)\n",
    "        print(val_loss_epoch)\n",
    "        print(\"*********************************\\n\")\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, train_loss_epoch, val_loss_epoch\n",
    "\n",
    "def test_model(model, dataloaders, dataset_sizes, criterion_vec):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_loss = 0.0\n",
    "    model.eval()   # Set model to evaluate mode\n",
    "    running_corrects = 0\n",
    "    running_loss = 0.0\n",
    "    num_outputs    = len(criterion_vec)\n",
    "    # Iterate over data.\n",
    "    for i, data in enumerate(dataloaders) :\n",
    "        inputs = data[0] \n",
    "        labels = data[1]\n",
    "        inputs, labels = convert_inputs(inputs, labels)\n",
    "        current_batch_size = inputs.size(0)\n",
    "        if(i%10 == 0) :\n",
    "            display_torch_im(inputs)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            regression_preds, classification_preds = \\\n",
    "                                  custom_preds(outputs, current_batch_size, num_outputs)\n",
    "            #custom loss for regression + classification head\n",
    "            loss, losses = custom_loss(outputs, labels)\n",
    "            if(i%10 == 0) :\n",
    "                print_model_output(\"preds\", regression_preds, classification_preds, 1)\n",
    "                print_model_label(\"labels\", labels, 1)\n",
    "                print(\"losses : \", losses)\n",
    "                print(\"loss   : \", loss)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc  = 0.0\n",
    "    epoch_loss = running_loss / dataset_sizes\n",
    "    print('Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                epoch_loss, epoch_acc))\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Test complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augmentations():\n",
    "    # applies the given augmenter in 50% of all cases,\n",
    "    sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "\n",
    "    # Define our sequence of augmentation steps that will be applied to every image\n",
    "    seq = iaa.Sequential([\n",
    "            # execute 0 to 5 of the following (less important) augmenters per image\n",
    "            iaa.SomeOf((0, 5),\n",
    "                [\n",
    "                    iaa.OneOf([\n",
    "                        iaa.GaussianBlur((0, 3.0)),\n",
    "                        iaa.AverageBlur(k=(2, 7)),\n",
    "                        iaa.MedianBlur(k=(3, 11)),\n",
    "                    ]),\n",
    "                    iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)),\n",
    "                    iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)),\n",
    "                    # search either for all edges or for directed edges,\n",
    "                    # blend the result with the original image using a blobby mask\n",
    "                    iaa.SimplexNoiseAlpha(iaa.OneOf([\n",
    "                        iaa.EdgeDetect(alpha=(0.5, 1.0)),\n",
    "                        iaa.DirectedEdgeDetect(alpha=(0.5, 1.0), direction=(0.0, 1.0)),\n",
    "                    ])),\n",
    "                    iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n",
    "                    iaa.OneOf([\n",
    "                        iaa.Dropout((0.01, 0.1), per_channel=0.5), # randomly remove up to 10% of the pixels\n",
    "                        iaa.CoarseDropout((0.03, 0.15), size_percent=(0.02, 0.05), per_channel=0.2),\n",
    "                    ]),\n",
    "                    iaa.Add((-10, 10), per_channel=0.5), # change brightness of images (by -10 to 10 of original value)\n",
    "                    iaa.AddToHueAndSaturation((-20, 20)), # change hue and saturation\n",
    "                    # either change the brightness of the whole image (sometimes\n",
    "                    # per channel) or change the brightness of subareas\n",
    "                    iaa.OneOf([\n",
    "                        iaa.Multiply((0.5, 1.5), per_channel=0.5),\n",
    "                        iaa.FrequencyNoiseAlpha(\n",
    "                            exponent=(-4, 0),\n",
    "                            first=iaa.Multiply((0.5, 1.5), per_channel=True),\n",
    "                            second=iaa.ContrastNormalization((0.5, 2.0))\n",
    "                        )\n",
    "                    ]),\n",
    "                    iaa.ContrastNormalization((0.5, 2.0), per_channel=0.5), # improve or worsen the contrast\n",
    "                    sometimes(iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25)), # move pixels locally around (with random strengths)\n",
    "                ],\n",
    "                random_order=True\n",
    "            )\n",
    "        ],\n",
    "        random_order=True\n",
    "    )\n",
    "    return seq\n",
    "class Augment(object):\n",
    "    def __init__(self, seq):\n",
    "        self.seq = seq\n",
    "\n",
    "    def __call__(self, im):\n",
    "        return Image.fromarray(self.seq.augment_images([np.array(im)])[0])\n",
    "\n",
    "def find_label_max(pickle_data_l, pickle_data_r) : \n",
    "    count                = 0\n",
    "    max_relative_angle   = 0.0\n",
    "    l_max_centre_dist    = 0.0\n",
    "    r_max_centre_dist    = 0.0\n",
    "    for key, value in pickle_data_l.items() :\n",
    "        if(abs(value['relative_angle']) > max_relative_angle) : \n",
    "            max_relative_angle = abs(value['relative_angle'])                                         \n",
    "        if(abs(value['centre_dist']) > l_max_centre_dist): \n",
    "            l_max_centre_dist  = abs(value['centre_dist'])\n",
    "\n",
    "    for key, value in pickle_data_r.items() :\n",
    "        if(abs(value['relative_angle']) > max_relative_angle) : \n",
    "            max_relative_angle = abs(value['relative_angle'])                                         \n",
    "        if(abs(value['centre_dist']) > r_max_centre_dist): \n",
    "            r_max_centre_dist  = abs(value['centre_dist'])\n",
    "    print('max relative angle : ', max_relative_angle , \" max centre dist (l) : \", l_max_centre_dist, \" (r) : \", r_max_centre_dist)\n",
    "    return max_relative_angle, l_max_centre_dist, r_max_centre_dist\n",
    "\n",
    "class CARLA_Dataset(Dataset):\n",
    "    def __init__(self, t, pickle_file_path_l, image_dir_path, image_file_names, pickle_file_path_r):\n",
    "        #initializing the max range\n",
    "        self.max_rangle      = 0.0\n",
    "        self.max_centre_dist = 0.0\n",
    "        self.max_vdistance   = 50.0\n",
    "        #initializing transforms\n",
    "        print(\"T : \", t)\n",
    "        assert t in ['train', 'val']\n",
    "        self.transform = get_data_transforms(t)\n",
    "        ###############LEFT CAMERA##############\n",
    "        self.image_path = image_dir_path\n",
    "        self.image_list = os.listdir(image_dir_path)\n",
    "        self.file_name = image_file_names\n",
    "        #initialize the labels \n",
    "        #read pickle file\n",
    "        self.pickle_list_l = os.listdir(pickle_file_path_l)\n",
    "        self.pickled_data_l = {}\n",
    "        for file in self.pickle_list_l:\n",
    "            f = open((pickle_file_path_l + file), 'rb')  \n",
    "            self.pickled_data_l.update(pickle.load(f))\n",
    "            f.close() \n",
    "        print(self.image_list)\n",
    "        ###############RIGHT CAMERA##############\n",
    "        self.pickle_list_r = os.listdir(pickle_file_path_r)\n",
    "        self.pickled_data_r = {}\n",
    "        for file in self.pickle_list_r:\n",
    "            f = open((pickle_file_path_r + file), 'rb')  \n",
    "            self.pickled_data_r.update(pickle.load(f))\n",
    "            f.close() \n",
    "        f.close() \n",
    "        self.max_rangle, self.l_max_centre_dist, self.r_max_centre_dist \\\n",
    "            = find_label_max(self.pickled_data_l, self.pickled_data_r)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_name)\n",
    "\n",
    "    def normalize_labels(self, labels_dict, chk_camera) : \n",
    "        labels_dict['front_vehicle']  /= self.max_vdistance\n",
    "        labels_dict['relative_angle'] /= self.max_rangle\n",
    "        labels_dict['centre_dist']    /= self.r_max_centre_dist\n",
    "        return labels_dict\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        frames      = []\n",
    "        inputs = {}  \n",
    "        labels = {}\n",
    "        #####################LEFT CAMERA################\n",
    "        #reading PIL\n",
    "        image_name = self.file_name[idx] + '.png'\n",
    "        raw_image = Image.open(os.path.join(self.image_path, image_name)).convert('RGB')\n",
    "        #reading file name to access the pickle file\n",
    "        current_fname = self.file_name[idx]\n",
    "        chk_camera = current_fname[-3:]\n",
    "\n",
    "        if chk_camera == '_rt' :\n",
    "            current_fname_r = current_fname[:-3]\n",
    "            label_dict = self.pickled_data_r[current_fname_r] \n",
    "        else :\n",
    "            label_dict = self.pickled_data_l[current_fname] \n",
    "        label_values = list(label_dict.values())\n",
    "        #transforming regression labels\n",
    "        label_dict                    = self.normalize_labels(label_dict, chk_camera)\n",
    "        labels['front_vehicle']       = torch.Tensor(np.array(float(label_dict['front_vehicle'])))\n",
    "        labels['centre_dist']         = torch.Tensor(np.array(float(label_dict['centre_dist'])))\n",
    "        labels['pedestrian_distance'] = torch.Tensor(np.array(float(label_dict['pedestrian_distance'])))\n",
    "        labels_traffic_light          = get_traffic_light_one_hot(label_dict['traffic_light'])\n",
    "        labels['traffic_light']       = torch.Tensor(np.array(float(labels_traffic_light)))\n",
    "        labels['relative_angle']      = torch.Tensor(np.array(float(label_dict['relative_angle'])))\n",
    "        \n",
    "        im = self.transform(raw_image)\n",
    "        inputs ['sequence'] = im\n",
    "        return inputs, labels\n",
    "\n",
    "#dataset support functions\n",
    "def get_data_transforms(t='train'):\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "#             Augment(get_augmentations()),\n",
    "            Crop((0,120,800,480)),\n",
    "            Rescale(0.4),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            Crop((0,120,800,480)),\n",
    "            Rescale(0.4),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "    return data_transforms[t]\n",
    "\n",
    "def get_data(batch_size):\n",
    "    dataset_dir = NEW_DATASET_DIR\n",
    "    image_dir   = NEW_DATASET_DIR + 'Camera'\n",
    "    train_filenames, test_filenames = get_train_test_split(image_dir)\n",
    "    train_ds = CARLA_Dataset( 'train', pickle_file_path_l = dataset_dir+'Left_pickle_file/',\n",
    "                                       image_dir_path     = dataset_dir+'Camera/',\n",
    "                                       image_file_names   = train_filenames, \n",
    "                                       pickle_file_path_r = dataset_dir+'Right_pickle_file/')\n",
    "    val_ds   = CARLA_Dataset( 'val',   pickle_file_path_l = dataset_dir+'Left_pickle_file/',\n",
    "                                       image_dir_path     = dataset_dir+'Camera/',\n",
    "                                       image_file_names   = test_filenames, \n",
    "                                       pickle_file_path_r = dataset_dir+'Right_pickle_file/')\n",
    "    return (\n",
    "        len(train_ds),\n",
    "        len(val_ds),\n",
    "        DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=1),\n",
    "        DataLoader(val_ds, batch_size=batch_size*2, num_workers=1),\n",
    "        train_ds,\n",
    "        val_ds\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_train_ds, len_valid_ds, train_dl, valid_dl, train_ds, val_ds = get_data(batch_size=128)\n",
    "print(len_train_ds)\n",
    "print(len_valid_ds)\n",
    "\n",
    "dataloaders   = {'train':train_dl, 'val':valid_dl}\n",
    "print(dataloaders['train'])\n",
    "dataset_sizes = {'train':len_train_ds, 'val':len_valid_ds}\n",
    "print(\"dataset sizes :\", dataset_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model for combined for regression and classification prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_preds(output, batch_size, num_outputs) : \n",
    "#     print(\"custom preds : \", output.size(), batch_size, num_outputs)\n",
    "    num_pred_outputs = num_outputs-1\n",
    "    num_regression_outputs = 3\n",
    "    \n",
    "    regression_preds     = torch.zeros(batch_size, num_regression_outputs)\n",
    "    classification_preds = torch.zeros(batch_size) \n",
    "    \n",
    "    regression_preds        = output[:,0:3]\n",
    "    _, classification_preds = torch.max(output[:,3:5], 1) #.float()\n",
    "    classification_preds    = classification_preds.T.float()\n",
    "    classification_preds    = classification_preds.unsqueeze(dim=1)\n",
    "    return regression_preds, classification_preds\n",
    "    \n",
    "\n",
    "def custom_loss(output, target, criterion_dict=None, criterion_vec=None) : \n",
    "    regression_loss     = 0.0\n",
    "    classification_loss = 0.0\n",
    "    loss     = 0.0\n",
    "    criterion_mse       = nn.MSELoss()\n",
    "    criterion_bce_logit = nn.BCEWithLogitsLoss()\n",
    "    criterion_ce        = nn.CrossEntropyLoss()\n",
    "    losses              = []\n",
    "    for i in range(3) : \n",
    "        task_regression_loss = criterion_mse(output[:,i], target[:,i])\n",
    "        regression_loss     += task_regression_loss\n",
    "        losses.append(task_regression_loss.item())\n",
    "    classification_target = target[:,3].long()\n",
    "    classification_target = classification_target #.squeeze(dim=1)\n",
    "    classification_output = output[:,3:5]\n",
    "    classification_loss = criterion_ce(classification_output, classification_target)\n",
    "    losses.append(classification_loss.item())\n",
    "    loss = regression_loss + classification_loss\n",
    "\n",
    "    return loss, losses\n",
    "\n",
    "#model definitions (combined regression)\n",
    "def convert_inputs(inputs_old, labels):\n",
    "    inputs               = inputs_old['sequence']\n",
    "    labels_vdistance     = labels['front_vehicle'] #/50.0\n",
    "    labels_cdistance     = labels['centre_dist']\n",
    "    labels_direction     = labels['pedestrian_distance']\n",
    "    labels_rangle        = labels['relative_angle']\n",
    "    labels_traffic_light = labels['traffic_light']\n",
    "    labels_traffic_light = labels_traffic_light#.squeeze(dim=1)\n",
    "    combined_label       = torch.stack([labels_vdistance, labels_cdistance, labels_rangle, labels_traffic_light], dim=1)\n",
    "    return inputs, combined_label\n",
    "\n",
    "def print_model_output(name, regression_out, classification_out, num_output=5) : \n",
    "    print(\"****** \", name, \" ******\")\n",
    "    print(\"VD : \", regression_out[0:num_output,0].cpu().detach().numpy(), \\\n",
    "          \" \\nCD : \", regression_out[0:num_output,1].cpu().detach().numpy(), \\\n",
    "          \" \\nRA : \", regression_out[0:num_output,2].cpu().detach().numpy(), \\\n",
    "          \" \\nTL : \", classification_out[0:num_output].T.cpu().detach().numpy())\n",
    "    \n",
    "def print_model_label(name, label, num_output=5) :\n",
    "    print(\"****** \", name, \" ******\")\n",
    "    print(\"VD : \", label[0:num_output,0].cpu().detach().numpy(), \\\n",
    "          \" \\nCD : \", label[0:num_output,1].cpu().detach().numpy(), \\\n",
    "          \" \\nRA : \", label[0:num_output,2].cpu().detach().numpy(), \\\n",
    "          \" \\nTL : \", label[0:num_output,3].cpu().detach().numpy())\n",
    "\n",
    "\n",
    "device           = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion_vec    = [1,1,0,0]\n",
    "\n",
    "print(\"Choose CNN model (Enter No. ) : \")\n",
    "print(\"0. VGG16\")\n",
    "print(\"1. Resnet18\")\n",
    "print(\"2. Resnet50\")\n",
    "model_idx = int(input())\n",
    "\n",
    "if(model_idx < 0 or model_idx > 2) :\n",
    "    print(\"Incorrect model selected : \", model_idx, \" Expected (0/1/2)\")\n",
    "    \n",
    "if(model_idx == 0) : \n",
    "    print(\"choosing VGG16....\")\n",
    "    model_ft         = models.vgg16(pretrained=True)\n",
    "    model_ft.classifier[6].out_features = 5\n",
    "elif(model_idx == 1) : \n",
    "    print(\"choosing Resnet18....\")\n",
    "    model_ft         = models.resnet18(pretrained=True)\n",
    "    num_ftrs         = model_ft.fc.in_features\n",
    "    model_ft.fc      = nn.Linear(num_ftrs, 5)\n",
    "else : \n",
    "    print(\"choosing Resnet50....\")\n",
    "    model_ft         = models.resnet50(pretrained=True)\n",
    "    num_ftrs         = model_ft.fc.in_features\n",
    "    model_ft.fc      = nn.Linear(num_ftrs, 5)\n",
    "model_ft         = nn.DataParallel(model_ft)\n",
    "model_ft         = model_ft.to(device)    \n",
    "optimizer_ft     = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # #train model\n",
    "# print(\"starting training...\")\n",
    "# # #if(args.retrain == True) : \n",
    "# #  #   resume_model(args.checkpoint, model_ft, optimizer_ft)\n",
    "# resume_model(ROOT_DIR + 'checkpoints/model_14.tar', model_ft, optimizer_ft) \n",
    "criterion_dict     = {'MSE' : nn.MSELoss, 'CE' : nn.CrossEntropyLoss}\n",
    "model_final, train_loss_epoch, val_loss_epoch = train_model(model_ft, dataloaders, dataset_sizes, criterion_dict, criterion_vec, optimizer_ft, exp_lr_scheduler, 15)\n",
    "print(train_loss_epoch)\n",
    "print(val_loss_epoch)\n",
    "\n",
    "dump_name = input(\"Enter test name\")\n",
    "np.savez('../dumps/'+dump_name+'.npz', train_loss=train_loss_epoch, val_loss_epoch=val_loss_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_checkpoint_weights(ROOT_DIR +'checkpoints/resnet18_all_affordance/model_16.tar', model_ft)\n",
    "test_model(model_ft, dataloaders['val'], dataset_sizes['val'], criterion_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #model definitions (combined regression)\n",
    "# def convert_inputs(inputs_old, labels):\n",
    "#     inputs               = inputs_old['sequence']\n",
    "#     labels_vdistance     = labels['front_vehicle']/50\n",
    "#     labels_cdistance     = labels['centre_dist']\n",
    "#     labels_direction     = labels['pedestrian_distance']\n",
    "#     labels_traffic_light = labels['traffic_light']\n",
    "# #     labels_traffic_light = labels_traffic_light.T\n",
    "#     #print(\"driection shape : \", labels_direction.size())\n",
    "#     print(\"labels traffic before : \", labels_traffic_light.size())\n",
    "#     labels_traffic_light = labels_traffic_light.squeeze(dim=1)\n",
    "#     print(\"labels traffic after : \", labels_traffic_light.size())\n",
    "# #     combined_label       = torch.stack([labels_vdistance, labels_cdistance, labels_traffic_light], dim=1)\n",
    "#     combined_label       = torch.stack([labels_vdistance, labels_cdistance], dim=1)\n",
    "    \n",
    "#     print(combined_label)\n",
    "#     return inputs, combined_label\n",
    "\n",
    "# train_dataloader                 = dataloaders['train']\n",
    "# data                             = next(iter(train_dataloader))\n",
    "# test_input , test_train_label    = data\n",
    "# converted_input, converted_label = convert_inputs(test_input, test_train_label)\n",
    "\n",
    "# test_train_im = test_input['sequence']\n",
    "# print(test_train_label)\n",
    "# print(test_train_im.size())\n",
    "# for i in range(16) :\n",
    "#     print(test_train_label['front_vehicle'][i])\n",
    "#     print(\"converted label : \", converted_label[i])\n",
    "#     plot_tensor_image(test_train_im[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

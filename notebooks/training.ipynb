{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8YSJ_MuKSRJN"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import pickle\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dWSZYEt_1j-s"
   },
   "outputs": [],
   "source": [
    "def onehot(vals, possible_vals):\n",
    "    if not isinstance(possible_vals, list): raise TypeError(\"provide possible_vals as a list\")\n",
    "    enc_vals = np.zeros([len(vals), len(possible_vals)])\n",
    "    for i, value in enumerate(vals):\n",
    "        if isinstance(possible_vals[0], float):\n",
    "            enc = np.where(abs(possible_vals-value)<1e-3)\n",
    "        else:\n",
    "            enc = np.where(possible_vals==value)\n",
    "        enc_vals[i,enc] = 1\n",
    "    return enc_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augmentations():\n",
    "    # applies the given augmenter in 50% of all cases,\n",
    "    sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "\n",
    "    seq = iaa.Sequential([\n",
    "            iaa.SomeOf((0, 4),\n",
    "                [\n",
    "                    iaa.OneOf([\n",
    "                        iaa.GaussianBlur((0, 3.0)),\n",
    "                        iaa.AverageBlur(k=(2, 7)), \n",
    "                        iaa.MedianBlur(k=(3, 11)),\n",
    "                    ]),\n",
    "                    iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)),\n",
    "                    iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)), \n",
    "                    # generate continuous masks following simplex noise and uses them to perform local blending\n",
    "                    iaa.SimplexNoiseAlpha(iaa.OneOf([\n",
    "                        iaa.EdgeDetect(alpha=(0.5, 1.0)),\n",
    "                        iaa.DirectedEdgeDetect(alpha=(0.5, 1.0), direction=(0.0, 1.0)),\n",
    "                    ])),  \n",
    "                ],\n",
    "                random_order=True\n",
    "            )\n",
    "        ],\n",
    "        random_order=True\n",
    "    )\n",
    "    return seq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I5kdFmDunl-3"
   },
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    def __init__(self, scalar):\n",
    "        self.scalar = scalar\n",
    "\n",
    "    def __call__(self, im):\n",
    "        w, h = [int(s*self.scalar) for s in im.size]\n",
    "        return transforms.Resize((h, w))(im)\n",
    "\n",
    "class Crop(object):\n",
    "    def __init__(self, box):\n",
    "        assert len(box) == 4\n",
    "        self.box = box\n",
    "\n",
    "    def __call__(self, im):\n",
    "        return im.crop(self.box)\n",
    "\n",
    "class Augment(object):\n",
    "    def __init__(self, seq):\n",
    "        self.seq = seq\n",
    "\n",
    "    def __call__(self, im):\n",
    "        return Image.fromarray(self.seq.augment_images([np.array(im)])[0])\n",
    "\n",
    "def get_data_transforms(t='train'):\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            Augment(get_augmentations()),\n",
    "            #Crop((0,120,800,480)),\n",
    "            #Rescale(0.4),\n",
    "            #transforms.ToTensor(),\n",
    "            #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            Augment(get_augmentations()),\n",
    "            Crop((0,120,800,480)),\n",
    "            Rescale(0.4),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "    return data_transforms[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YWlWnMvlY2is"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from matplotlib import image\n",
    "from matplotlib import pyplot\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "#import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "#LABEL_KEYS = ['v_throttle', 'v_break', 'v_steer', 'traffic_light', 'front_vehicle', 'centre_dist', 'pedestrian_distance']\n",
    "\n",
    "class CARLA_Dataset(Dataset):\n",
    "    def __init__(self, t, pickle_file_path_l, image_dir_path, pickle_file_path_r):\n",
    "       # self.i = 0\n",
    "      #  self.j = 0\n",
    "        #self.inputs, self.labels = {}, {}\n",
    "        #initializing transforms\n",
    "        assert t in ['train', 'val']\n",
    "        self.transform = get_data_transforms(t)   \n",
    "        ###############LEFT CAMERA##############\n",
    "        #reading the title of all images to access the pickle file\n",
    "        self.image_path = image_dir_path\n",
    "        self.image_list = os.listdir(image_dir_path)\n",
    "        #self.image_list_l.remove('.DS_Store')\n",
    "        self.file_name = []\n",
    "        for file in self.image_list:\n",
    "            self.file_name.append(os.path.splitext(file)[0])\n",
    "            \n",
    "        #initialize the labels \n",
    "\n",
    "        \n",
    "        #read pickle file\n",
    "        self.pickle_list_l = os.listdir(pickle_file_path_l)\n",
    "        self.pickled_data_l = {}\n",
    "        for file in self.pickle_list_l:\n",
    "            f = open((pickle_file_path_l + file), 'rb')  \n",
    "            self.pickled_data_l.update(pickle.load(f))\n",
    "\n",
    "            f.close() \n",
    "        \n",
    "        ###############RIGHT CAMERA##############\n",
    "    \n",
    "        \n",
    "        #read pickle file\n",
    "        self.pickle_list_r = os.listdir(pickle_file_path_r)\n",
    "        self.pickled_data_r = {}\n",
    "        for file in self.pickle_list_r:\n",
    "            f = open((pickle_file_path_r + file), 'rb')  \n",
    "            self.pickled_data_r.update(pickle.load(f))\n",
    "            f.close() \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_name)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "       # try:\n",
    "            \n",
    "            frames      = []\n",
    "            inputs = {}\n",
    "            labels = {}    \n",
    "            #####################LEFT CAMERA################\n",
    "            #reading PIL\n",
    "            self.raw_image = Image.open(os.path.join(self.image_path, self.image_list[idx])).convert('RGB')\n",
    "            #pyplot.imshow(self.raw_image_l)\n",
    "            #pyplot.show()\n",
    "\n",
    "            #reading file name to access the pickle file\n",
    "            current_fname = self.file_name[idx]\n",
    "            chk_camera = current_fname[-3:]\n",
    "\n",
    "            if chk_camera == '_rt' :\n",
    "                current_fname_r = current_fname[:-3]\n",
    "                label_dict = self.pickled_data_r[current_fname_r] \n",
    "            else :\n",
    "\n",
    "                label_dict = self.pickled_data_l[current_fname] \n",
    "            label_values = list(label_dict.values())\n",
    "        \n",
    "            #transforming regression labels\n",
    "            labels['front_vehicle'] = torch.Tensor(np.array((float(label_dict['front_vehicle'])/50)))\n",
    "            labels['centre_dist'] = torch.Tensor(np.array(float(label_dict['centre_dist'])))\n",
    "            labels['relative_angle'] = torch.Tensor(np.array(float(label_dict['relative_angle'])))\n",
    "            #print(label_dict['front_vehicle'])\n",
    "            #print(labels['front_vehicle'])\n",
    "            #transforming classification labels\n",
    "            #self.labels_l['traffic_light'] = torch.Tensor(onehot(np.array(label_dict['traffic_light']), [False, 'Green', True, 'Red']))\n",
    "            #transforming raw PIL image\n",
    "            print(\"Original Image:\")\n",
    "            pyplot.imshow(self.raw_image)\n",
    "            pyplot.show()\n",
    "            im = self.transform(self.raw_image)\n",
    "            #self.j += 1\n",
    "            #print(self.j)\n",
    "            print(\"Crop:\")\n",
    "            pyplot.imshow(im)\n",
    "            pyplot.show()\n",
    "            #inputs ['sequence'] = im\n",
    "            #print(inputs ['sequence'].dim())\n",
    "            return inputs, labels\n",
    "        \n",
    "       # except KeyError:\n",
    "        #    self.i += 1\n",
    "        #    os.remove('/home/seima/carla/CARLA_0.9.6/PythonAPI/examples/Validation_dataset/camera_images/' + current_fname + '.png')\n",
    "         #   print(\"removed:\" ,self.i)\n",
    "        #    pass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1140,
     "status": "ok",
     "timestamp": 1572580450204,
     "user": {
      "displayName": "Seima Saki",
      "photoUrl": "",
      "userId": "02501578531897632617"
     },
     "user_tz": -480
    },
    "id": "pg-IJ8D-_x1S",
    "outputId": "657a685a-8200-44e4-d42f-6db08adee6eb"
   },
   "outputs": [],
   "source": [
    "C_dataset = CARLA_Dataset( 'train', pickle_file_path_l='/home/seima/carla/CARLA_0.9.6/PythonAPI/examples/Validation_dataset/Left_pickle_file/',\n",
    "                                    image_dir_path='/home/seima/carla/CARLA_0.9.6/PythonAPI/examples/Validation_dataset/camera_images/', \n",
    "                                    pickle_file_path_r='/home/seima/carla/CARLA_0.9.6/PythonAPI/examples/Validation_dataset/Right_pickle_file/')\n",
    "\n",
    "for i in range(len(C_dataset)):\n",
    "    sample = C_dataset[i]\n",
    "    #print(sample[0])\n",
    " #print(sample['labels_l'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(batch_size):\n",
    "    train_ds = CARLA_Dataset( 'train', pickle_file_path_l='/home/seima/carla/CARLA_0.9.6/PythonAPI/examples/Validation_dataset/Left_pickle_file/',\n",
    "                                    image_dir_path='/home/seima/carla/CARLA_0.9.6/PythonAPI/examples/Validation_dataset/camera_images/', \n",
    "                                    pickle_file_path_r='/home/seima/carla/CARLA_0.9.6/PythonAPI/examples/Validation_dataset/Right_pickle_file/')\n",
    "    val_ds = CARLA_Dataset( 'val', pickle_file_path_l='/home/seima/carla/CARLA_0.9.6/PythonAPI/examples/Validation_dataset/Left_pickle_file/',\n",
    "                                    image_dir_path='/home/seima/carla/CARLA_0.9.6/PythonAPI/examples/Validation_dataset/camera_images/', \n",
    "                                    pickle_file_path_r='/home/seima/carla/CARLA_0.9.6/PythonAPI/examples/Validation_dataset/Right_pickle_file/')\n",
    "    return (len(train_ds),len(val_ds),DataLoader(train_ds, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=10),\n",
    "        DataLoader(val_ds, batch_size=batch_size*2, pin_memory=True, num_workers=10),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(get_data(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##########################################################\n",
    "# pytorch util functions \n",
    "##########################################################\n",
    "\n",
    "def load_checkpoint(filename) :\n",
    "    #print(filename)\n",
    "    checkpoint = torch.load(filename)\n",
    "    model_state_dict = checkpoint['model_state_dict']\n",
    "    optimizer_state_dict = checkpoint['optimizer_state_dict']\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    return [model_state_dict, optimizer_state_dict, epoch, loss]\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, loss, filename):\n",
    "    print(\"Saving Checkpoint....\")\n",
    "    model_state_dict = model.state_dict()\n",
    "    optimizer_state_dict = optimizer.state_dict()\n",
    "#     print(\"model_state_dict : \", model_state_dict)\n",
    "#     print(\"optimizer_state_dict : \", optimizer_state_dict)\n",
    "#     print(\"Epoch : \", epoch)\n",
    "#     print(\"filename : \", filename)\n",
    "\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model_state_dict,\n",
    "            'optimizer_state_dict': optimizer_state_dict,\n",
    "            'loss': loss\n",
    "            }, filename)\n",
    "    print(\"Saving Checkpoint Done\")\n",
    "\n",
    "def display_image(image) : \n",
    "    plt.show(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "#from ipdb import set_trace\n",
    "from datetime import datetime\n",
    "import time\n",
    "import copy\n",
    "#from util_funcs import *\n",
    "from tqdm import tqdm\n",
    "from ipdb import set_trace\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#sys.path.insert(0, \"../prev_work/CAL/training\")\n",
    "\n",
    "#from train import fit, custom_loss, validate\n",
    "#from metrics import calc_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_inputs(inputs_old, labels_tostack):\n",
    "    inputs            = inputs_old['sequence']\n",
    "    #print(\"labels keys : \", labels.keys())\n",
    "    #labels = list(sample['labels_l'].values())\n",
    "    labels_vdistance  = labels_tostack['front_vehicle']\n",
    "    labels_cdistance  = labels_tostack['centre_dist']\n",
    "    labels_relativeang  = labels_tostack['relative_angle']\n",
    "    #combined_label    = torch.stack([labels_vdistance, labels_cdistance, labels_relativeang], dim=1)\n",
    "    combined_label    = torch.stack([labels_vdistance], dim=1)\n",
    "    #print(\"combined_label shape : \", combined_label.size())\n",
    "    return inputs, combined_label\n",
    "    #return combined_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, dataset_sizes, criterion, optimizer, scheduler, num_epochs=30):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            #for inputs, labels in dataloaders[phase]:\n",
    "            for i, data in enumerate(dataloaders[phase]) :\n",
    "            #for i, data in tqdm(dataloaders[phase]) :\n",
    "                inputs_old = data[0]\n",
    "                #print(\"input before convertion\", inputs_old)\n",
    "                labels = data[1]\n",
    "                inputs, labels = convert_inputs(inputs_old ,labels)\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    #_, preds = torch.max(outputs, 1)\n",
    "                    preds = outputs\n",
    "                    #print(outputs.size(), labels.size())\n",
    "                    #input(\"Enter\")\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        if(i%100 == 0) :\n",
    "                            print(\"preds shape : \", preds.size(), \" labels shape : \", labels.size())\n",
    "                            print(\"preds : \", preds[0].T.data, \"\\nlabels : \", labels.data[0].T.data)\n",
    "                            print(\"loss :\", loss.item())\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                #running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                if(phase == 'val') :\n",
    "                    if(i%10==0) :\n",
    "                        print(\"preds : \", preds[0].T.data, \"\\nlabels : \", labels.data[0].T.data)\n",
    "                        print(\"loss :\", loss.item())\n",
    "                #if(i==10) : \n",
    "                #    break\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            #epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            #if phase == 'val' and epoch_acc > best_acc:\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                #best_acc = epoch_acc\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'train'  :\n",
    "                checkpoint_name = 'checkpoints/model_' + str(epoch) + '.tar'\n",
    "                save_checkpoint(model, optimizer, epoch, epoch_loss, checkpoint_name)\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint_weights(checkpoint, model) :\n",
    "    print(\"checkpoint name : \", checkpoint)\n",
    "    _model_state_dict, _optimizer_state_dict, _epoch, _loss = load_checkpoint(checkpoint)\n",
    "    model.load_state_dict(_model_state_dict)\n",
    "\n",
    "def resume_model(checkpoint, model, optimizer) : \n",
    "    print(\"checkpoint name : \", checkpoint)\n",
    "    _model_state_dict, _optimizer_state_dict, _epoch, _loss = load_checkpoint(checkpoint)\n",
    "    #print(_model_state_dict)\n",
    "    model.load_state_dict(_model_state_dict)\n",
    "    optimizer.load_state_dict(_optimizer_state_dict)\n",
    "    print(\"resume model succesful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_train_ds, len_valid_ds, train_dl, valid_dl = get_data(batch_size=64)\n",
    "print(len_train_ds)\n",
    "print(len_valid_ds)\n",
    "\n",
    "dataloaders   = {'train':train_dl, 'val':valid_dl}\n",
    "print(dataloaders['train'])\n",
    "dataset_sizes = {'train':len_train_ds, 'val':len_valid_ds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"dataset sizes :\", dataset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model definitions\n",
    "model_ft         = models.resnet50(pretrained=True)\n",
    "criterion        = nn.MSELoss()\n",
    "optimizer_ft     = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "#load_checkpoint_weights(args.checkpoint, model_ft)\n",
    "num_ftrs         = model_ft.fc.in_features\n",
    "model_ft.fc      = nn.Linear(num_ftrs, 1)\n",
    "model_ft         = nn.DataParallel(model_ft)\n",
    "model_ft         = model_ft.to(device)\n",
    "print(\"copying model to device....\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model\n",
    "print(\"starting training...\")\n",
    "#if(args.retrain == True) : \n",
    " #   resume_model(args.checkpoint, model_ft, optimizer_ft)\n",
    "train_model(model_ft, dataloaders, dataset_sizes, criterion, optimizer_ft, exp_lr_scheduler, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "data_loader.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
